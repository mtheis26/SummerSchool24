{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hackathon Day 1\n",
    "### Data Loading and Exploration\n",
    "\n",
    "In the first day of our Hackathon, we learn about loading and preprocessing the data and get some insights in the data structure and explemplary instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Select a dataset and initialize it\n",
    "\n",
    "As the first step, we need to import the data and initialize the Dataset object. It handles the download and the access to single data instances. A second important step when using data for machine learning is to think about a proper preprocessing pipeline. Raw data often has value ranges or data structured that need to be changed before it can be used in machine learning models. A convenient package to define preprocessing transformations is `torchvision` (https://pytorch.org/vision/)\n",
    "\n",
    "* Import the PneumoniaMNIST data with the `medmnist` package (https://medmnist.com/) and initialize it using preprocessing.\n",
    "* The PneumoniaMNIST dataset comes with several image sizes. Find out which sizes are available and visualize the differences.\n",
    "\n",
    "The MNIST datasets (as also many of other common data sets) have 3 so-called \"splits\".\n",
    "\n",
    "* What are the 3 splits of the data set?\n",
    "* What is the purpose of each split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2 as transforms\n",
    "import torch\n",
    "\n",
    "# IMPORT PNEUMONIA MNIST MODULE FROM MEDMNIST HERE\n",
    "\n",
    "# preprocessing\n",
    "TRANSFORM = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToImage(),\n",
    "        # the `Compose` function can take a list of transformations (also called pipeline)\n",
    "        # think of necessary prepreocessing functions here and add them to the list\n",
    "        # the basic transforms will convert the images to suitable data types and normalize the images\n",
    "        # SPECIFY LIST_OF_TRANSFORMS HERE\n",
    "    ]\n",
    ")\n",
    "\n",
    "# simple initialization of training-, validation-, and test-set:\n",
    "# DOWNLOAD AND APPLY THE TRANSFORMS TO THE PNEUMONIAMNIST DATASET.\n",
    "train_dataset = PneumoniaMNIST(split=SPLIT_NAME, transform=TRANSFORM, download=True, size=SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Visualize the dataset\n",
    "\n",
    "A visual exploration of the data should be one of your first steps when conducting a machine learning project. It helps to get familiar with the data structure and quality. Especially for use-case-tailored modeling task, it is essential to have a broad knowledge about the properties and characteristics of your data.\n",
    "\n",
    "* Print out some properties of the datasets initialized before\n",
    "* Visualize some example instances by using the `medmnist` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE PROPERTIES AND QUALITATIVE SAMPLES OF TRAINING-SET:\n",
    "print(\"Training Dataset:\")\n",
    "print(DATASET)\n",
    "\n",
    "# PLOT SOME SAMPLES FROM THE DATASET FOR TRAINING AND EVALUATION SPLITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the dataset information shows, we have a binary classification task. The label '0' represents a normal chest X-Ray, whereas a label '1' represents a chest X-Ray where pneumonia is visible. Thus, it might be useful to see, if we already see differences in instances that have different labels.\n",
    "\n",
    "* Write the code that visualizes 10 examples images from the train dataset for 'normal' and 'pneumonia' labels each using the `matplotlib` package (https://matplotlib.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, axes = plt.subplots(2,10, figsize=(12, 6))\n",
    "for n in range(2):\n",
    "    count=0\n",
    "    for img, label in train_dataset:\n",
    "        if label[0]==n and count<10:\n",
    "            # PLOT_SAMPLES here\n",
    "            count+=1\n",
    "\n",
    "# Set titles for the rows\n",
    "fig.text(0.5, 0.76, \"normal\", ha='center', fontsize=12)\n",
    "fig.text(0.5, 0.4, \"pneumonia\", ha='center', fontsize=12)\n",
    "\n",
    "# Adjust layout to ensure titles don't overlap with images\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you see significant differences between both labels? Briefly explain your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Plot class distributions\n",
    "\n",
    "Another recommendable staight-forward exploration method is to have a look into the class label distributions, so how many instances are available for each label.\n",
    "\n",
    "* Find a way to extract the labels for each of the 3 data sets (train, validation, test)\n",
    "* Count the labels for each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Helper function to extract the labels of the dataset\n",
    "def get_labels(dataset):\n",
    "    labels = []\n",
    "    for _, label in dataset:\n",
    "        labels.extend(label)\n",
    "    return np.array(labels)\n",
    "\n",
    "\n",
    "# EXTRACT LABELS FOR EACH SPLIT\n",
    "labels = get_labels(DATASET)\n",
    "# REPEAT FOR THE EVALUATION SPLITS\n",
    "\n",
    "# CALCULATE CLASS COUNTS OF PNEUMONIA/NORMAL SAMPLES FOR EACH SPLIT\n",
    "# HINT: you can use numpy ready to use function to calculate the unique count of values\n",
    "SPLIT_COUNT = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have the label counts, we want to visualize them for example as a bar plot. Again, use `matplotlib` for that.\n",
    "\n",
    "* \"easy\" task: Make a separate class distribution bar plot for each set\n",
    "* \"advanced\" task: Make a stacked bar plot for all class distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels for plotting\n",
    "labels_names = [\"Normal\", \"Pneumonia\"]\n",
    "\n",
    "# Easy: Create single bar plots\n",
    "for counts, split in zip([.., .., ..], [\"train\", \"validation\", \"test\"]):\n",
    "    fig, ax = plt.subplots()\n",
    "    #PLOT BARPLOT HERE\n",
    "    ax.set_xlabel(\"Class\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(f\"Class distribution for {split} set\")\n",
    "    plt.show()\n",
    "\n",
    "# Advanced: Create a stacked bar plot\n",
    "fig, ax = plt.subplots()\n",
    "#PLOT BARPLOT HERE\n",
    "\n",
    "ax.set_xlabel(\"Class\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Class distribution\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discribe your findings\n",
    "\n",
    "* Can you see something remarkable?\n",
    "* How are the labels distributed?\n",
    "* Are there implications / difficulties that may occur during model training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 (optional): Data augmentation\n",
    "\n",
    "#### Torchvision Augmentation\n",
    "\n",
    "For the very most machine learning problems it is: the more data, the better. The higher the variety of the data, the more knowledge the model can gain from it which increases its accuracy. Of course, the total amount of data is limited and especially in medical imaging, data acquisition is fairly expensive. However, there are methods to increase the dataset size by artificial instances that are \"close\" or \"similar\" to the real data instances by manipulation of the existing real data. This principle is known as *Data Augmentation*.\n",
    "\n",
    "In the field of (medical) image processing, we can augment our data e.g. by rotating, mirroring, or cropping existing images. Also changes in brightness, contrast, resolution, or adding random noise are possible. Basically, any augmentation method that leads to realistic representation and transformations of the real data is helpful.\n",
    "\n",
    "In general torchvision transforms (https://pytorch.org/vision/stable/transforms.html) provide a module for data transformations and augmentation. The torchvision transforms can be composed in a seuential transform and applied on the fly when passed to the dataset object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORM = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToImage(),\n",
    "        # include list of transformations for data augmentation\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset = PneumoniaMNIST(split=SPLIT_NAME, transform=TRANSFORM, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if the images are transformed in the dataset on the fly, sample some images from the dataset multiple times and check if you observe the differences in the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "# we prepare a grid where N_ROWS images are sampled from the dataset N_COLS times\n",
    "# SPECIFY THE N_ROWS AND N_COLS VALUES HERE\n",
    "grid = ImageGrid(fig, 111, nrows_ncols=(N_ROWS, N_COLS), axes_pad=0.1)\n",
    "\n",
    "sample_images = [train_dataset[row_idx][0] for row_idx in range(N_ROWS) for col_idx in range(N_COLS)]\n",
    "for ax, im in zip(grid, sample_images):\n",
    "    # PLOT IMAGE HERE\n",
    "    ax.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MONAI augmentation\n",
    "\n",
    "Especially for medial imaging, `MONAI` (https://monai.io/) is a powerful Python framework for machine learning in medical applications and, in particular, also provides image augmentation methods. Refer here https://docs.monai.io/en/stable/transforms.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import monai libraries\n",
    "\n",
    "import random\n",
    "from monai.data import CacheDataset\n",
    "from monai.transforms import Compose, EnsureTyped, ScaleIntensityd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = Compose([EnsureTyped(keys=[\"image\"], data_type=\"tensor\"), ScaleIntensityd(keys=[\"image\"], minv=0, maxv=1)])\n",
    "\n",
    "\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        # INCLUDE MONAI TRANSFORMATIONS HERE\n",
    "        # HINT: You can also randomly rotate or flip your dataset, apply gaussian noise, etc. to create more variability in the dataset\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# If you want to use Monai data transformations you have to use a wrapper class on PneumoniaMNIST to return the images and respective labels in the MONAI compatible format\n",
    "class WrappedPneumoniaMNIST(PneumoniaMNIST):\n",
    "    def __getitem__(self, index):\n",
    "        image, label = super().__getitem__(index)\n",
    "        image = torch.tensor(np.array(image)).unsqueeze(0)\n",
    "        return {\"image\": image, \"label\": label}\n",
    "\n",
    "\n",
    "# Load Pneumonia dataset for training, validation and test\n",
    "size = (\n",
    "    ...\n",
    ")  # for first experiments you can use 28x28 to get fast results but in the end you should work with a higher resolution, e.g. 224x224\n",
    "train_dataset = WrappedPneumoniaMNIST(split=\"train\", download=True, size=size)\n",
    "val_dataset = ...\n",
    "test_dataset = ...\n",
    "\n",
    "# Wrap with MONAI CacheDataset\n",
    "# Attention! If you use transformations to augemnt your dataset, only apply it to the training data and not to the validation and test set, as we want to leave these unchanged.\n",
    "train_monai_dataset = CacheDataset(data=train_dataset, transform=train_transforms, cache_rate=1.0, num_workers=4)\n",
    "val_monai_dataset = ...\n",
    "test_monai_dataset = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize data augmentation by selecting random samples from the dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show 10 images\n",
    "num_samples = 10\n",
    "\n",
    "# CREATE A LIST WITH 10 RANDOM INDICES\n",
    "random_indices = random.sample(...)\n",
    "\n",
    "fig, axs = plt.subplots(2, num_samples, figsize=(20, 5))\n",
    "\n",
    "# HINT: For a specific index *idx* you can call the original data using dataset.data.imgs[idx] and the transformed data using dataset[idx]['image']\n",
    "for idx in random_indices:\n",
    "    orig_img = ...\n",
    "    transformed_img = ...\n",
    "\n",
    "    # SHOW IMAGE\n",
    "    axs[0, idx].imshow(orig_img, cmap=\"gray\")\n",
    "    axs[1, idx].imshow(transformed_img.squeeze(), cmap=\"gray\")\n",
    "\n",
    "    # ASSIGN TITLE TO THE PLOT AND THE INDIVIDUAL IMAGES AS NORMAL/PNEUMONIA\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Dataloader \n",
    "\n",
    "Training on large datasets all at once may exceed memory limits. Batching allows models to handle smaller chunks of data, fitting them into memory more easily. A DataLoader efficiently loads and manages data by handling batching, shuffling, and parallel processing. It feeds data into models during training and evaluation. Usually, we provide a dataset and batch size to the dataloader to create mini-batches out of the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORT DATALOADER FROM PYTORCH DATA MODULE \n",
    "from .. import DataLoader\n",
    "\n",
    "# PREPARE THE LOADERS FOR TRAIN AND EVALUATION SPLITS\n",
    "#SPECIFY THE APPROPRIATE ARGUMENTS HERE\n",
    "\n",
    "# please check what arguments might be specified differently for the training and evaluation dataloaders?  \n",
    "train_dataloader = DataLoader(dataset=..,\n",
    "                              batch_size= ..,\n",
    "                              shuffle=..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6 (optional): ChestMNIST\n",
    "\n",
    "Now, we have a look at another `medmnist` dataset - ChestMNIST.\n",
    "\n",
    "* Initialize the ChestMNIST data sets with a preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT ChestMNIST\n",
    "\n",
    "# preprocessing\n",
    "data_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToImage(),\n",
    "       # INCLUDE A LIST OF TRANSFORMS HERE\n",
    "    ]\n",
    ")\n",
    "\n",
    "# INITIALIZE TRAINING AND EVALUATION DATASETS FROM CHESTMNIST\n",
    "train_dataset = ChestMNIST(split=.., transform=.., download=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out some data set properties and example images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE PROPERTIES AND QUALITATIVE SAMPLES OF TRAINING-SET:\n",
    "print(\"Training Dataset:\")\n",
    "print(DATASET)\n",
    "\n",
    "# VISUALIZE THE VALIDATION AND TEST SETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What difference can you observe in the ChestMNIST dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ChestMNIST dataset is a so-called \"multi-label\" dataset.\n",
    "\n",
    "* What is the difference between a \"multi-label\" and a \"multi-class\" problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a look into how the labels are represented in the dataset.\n",
    "\n",
    "* Print out the labels of a few instances and find out, how the labels are represented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are represented in a so-calles \"multi-hot\" encoding. This is a binary label vector where a \"0\" means absence and \"1\" means presence of the respective disaese.\n",
    "\n",
    "* Why is that convenient if you think about model training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can count the label abundances but with another strategy than before.\n",
    "\n",
    "* Try to find a efficient way to calculate the counts of each label by avoiding explicit loops\n",
    "* Plot the results again in a stacked bar diagram\n",
    "* What can you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# CALCULATE CLASs COUNT FOR EACH SPLIT\n",
    "# HINT: you can use some numpy function \n",
    "train_counts = ..\n",
    "\n",
    "labels_names = .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced: Create a stacked bar plot\n",
    "fig, ax = plt.subplots()\n",
    "# CREATE STACKED BARPLOTOF DIFFERENT CLASSES INCLUDING TRAIN, VALIDTION AND TEST SPLITS\n",
    "\n",
    "ax.set_xlabel(\"Class\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Class distribution\")\n",
    "ax.tick_params(axis=\"x\", labelrotation=90)\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a multi-label problem here, it is hard to isolate one disease from another. However, we can do a more interesting thing! What about observing, if there are combinations of diseases that are more common than others or find correlations? A nice way of doing that is to calculate the correlation matrix. \n",
    "\n",
    "* Calculate the correlation matrix of the training labels.\n",
    "* Plot the correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the correlation matrix\n",
    "fig, ax = plt.subplots()\n",
    "cax = ax.matshow(corr_mat, cmap=\"coolwarm\")\n",
    "cbar = fig.colorbar(cax)\n",
    "cbar.set_label(\"Correlation coefficient\")\n",
    "ax.set_xticks(np.arange(len(labels_names)))\n",
    "ax.set_yticks(np.arange(len(labels_names)))\n",
    "ax.set_xticklabels(labels_names, rotation=90)\n",
    "ax.set_yticklabels(labels_names)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### if you are interested ...\n",
    "\n",
    "There are some more interesting Python packages for data exploration and manipulation than `numpy` and `matplotlib`. Here, we briefly mention some recommendable packages.\n",
    "\n",
    "#### [`pandas`](https://pandas.pydata.org)\n",
    "Pandas is a comprehensive library for data analysis with python. It offers efficient data structures and data manipulation methods for mainly numerical structured or tabular data. Additionally, it includes some visualization functions.\n",
    "\n",
    "#### [`seaborn`](https://seaborn.pydata.org)\n",
    "Seaborn is a library for statistical data visualization. It is based on matplotlib and complements it regarding convenience functions for plotting and advanced design options enabling efficient data exploration.\n",
    "\n",
    "#### [`scipy`](https://scipy.org)\n",
    "  SciPy is one of the fundamental libraries for scientific computation with Python. It offers lots of algorithms and functions for optimization, integration, interpolation, linear algebra, differential equations, etc.\n",
    "  \n",
    "#### [`scikit image`](https://scikit-image.org)\n",
    "  Scikit-image is a collection of algorithms for image processing and manipulation in Python. It offers functions in the context of image segmentation, geometric transforms, color manipulations, filtering, feature extraction and generation, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
