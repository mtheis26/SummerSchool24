{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hackathon Day 2\n",
    "Today we want to train a Convolutional Neural Network for classifying pneumonia on the [PneumoniaMNIST](https://medmnist.com/) dataset\n",
    "\n",
    "\n",
    "**Learning Outcome:** Learn how to\n",
    "- define training and model parameters and analyse their influence on model performance\n",
    "- write a training loop\n",
    "- evaluate classification performance using different performance metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Load dataset \n",
    "Use the code from day 1 to load your dataset.\\\n",
    "If you worked on this yesterday, you can also use data augmentation methods. Data augmentation is a technique used to increase the diversity of training data by applying random transformations, like rotating, flipping, or cropping images, to create new variations of existing data. This helps improve a model’s ability to generalize and perform better on unseen data.\\\n",
    "For data augmentation you can use the [MONAI library](https://docs.monai.io/en/stable/transforms.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some libraries we will need \n",
    "\n",
    "from medmnist import PneumoniaMNIST \n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn as nn\n",
    "from monai.data import DataLoader, CacheDataset\n",
    "from monai.utils import  GridSamplePadMode\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    EnsureTyped,\n",
    "    ScaleIntensityd\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = Compose(\n",
    "    [\n",
    "        EnsureTyped(keys=['image'], data_type='tensor'),\n",
    "        ScaleIntensityd(keys=['image'], minv=0, maxv=1 )\n",
    "    ] )\n",
    "\n",
    "# Optional: Apply Data Augmentation:\n",
    "# You can also randomly rotate or flip your dataset, apply gaussian noise, etc. to create more variability in the dataset \n",
    "# see https://docs.monai.io/en/stable/transforms.html\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        ...        \n",
    "    ])\n",
    "    \n",
    "# If you want to use Monai data transformations you have to use this class\n",
    "# to convert medmnist dataset to a format compatible with MONAI\n",
    "class WrappedPneumoniaMNIST(PneumoniaMNIST):\n",
    "    def __getitem__(self, index):\n",
    "        image, label = super().__getitem__(index)\n",
    "        image = torch.tensor(np.array(image)).unsqueeze(0)\n",
    "        return {\"image\": image, \"label\": label}\n",
    "\n",
    "# Load Pneumonia dataset for training, validation and test\n",
    "size=... # for first experiments you can use 28x28 to get fast results but in the end you should work with a higher resolution, e.g. 224x224\n",
    "train_dataset = WrappedPneumoniaMNIST(split=\"train\",download=True, size=size)\n",
    "val_dataset = ...\n",
    "test_dataset = ...\n",
    "\n",
    "# Wrap with MONAI CacheDataset\n",
    "# Attention! If you use transformations to augemnt your dataset, only apply it to the training data and not to the validation and test set, as we want to leave these unchanged.\n",
    "train_monai_dataset = CacheDataset(data=train_dataset, transform=train_transforms, cache_rate=1.0, num_workers=4)\n",
    "val_monai_dataset = ...\n",
    "test_monai_dataset = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next: initiliaze train_loader, val_loader and test_loader using DataLoader from MONAI\n",
    "BATCH_SIZE = ...\n",
    "train_loader = DataLoader()\n",
    "val_loader = ...\n",
    "test_loader = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Define training and model parameters\n",
    "- We want to train a classification model. You can for example use the **ResNet18** model. To initialize the model you can use [Monai](https://docs.monai.io/en/stable/networks.html#nets). \n",
    "- Moreover, we have to define our training parameters, as optimizer, loss function and learning rate. \n",
    "- For a binary classification problem you can for example use binary cross entropy loss (see [here](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html)). \n",
    "- As optimizer you can use stochastic gradient descent (see [here](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)) \n",
    "\n",
    "Carry out several experiments and vary the model parameters. What effect does this have on the model performance?\\\n",
    "You can use, for example, a larger model (e.g. ResNet 50), and vary the learning rate and the batch size. Write down your observations.  \n",
    "For quick experiments, you can work with the small data set (28x28). Ultimately, however, you should train a model on one of the higher-resolution data sets (64x64 or 224x224). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.networks.nets import resnet18\n",
    "\n",
    "device = torch.device(f'cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = resnet18().to(device) # use .to(device) to move the model to the gpu\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "lr=0.001 \n",
    "wd=0.001  \n",
    "# You can use weight decay as a penalty term for the model weights. \n",
    "# This penalty term prevents the weights from becoming to large and thus helps the model not only to memorize the training data, \n",
    "# but also to learn general patterns that can be applied to new data (prevents overfitting)\n",
    "\n",
    "optimizer = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Evaluation function \n",
    "- Use [scikit-learn](https://scikit-learn.org/stable/modules/model_evaluation.html) to evaluate your model\n",
    "- For a classification task you can for example use metrics like AUC, Accuracy, F1-Score, Sensitivity and Specificity\n",
    "\n",
    "What exactly do the individual metrics mean? How can you interpret the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score\n",
    "    ...\n",
    "    )\n",
    "\n",
    "def evalModel(y_true, y_score, thresh=0.5):\n",
    "    y_true = y_true.squeeze() # ground truth class / label\n",
    "    y_score = y_score.squeeze() # probabilities \n",
    "\n",
    "    y_pred = (y_score > thresh).astype(int) # predicted class\n",
    "\n",
    "    metric_dict = {}\n",
    "\n",
    "    metric_dict['Acc'] = accuracy_score(y_true, y_pred)\n",
    "    # calculate further evaluation metrics as AUC, F1-Score, sensitivity and specificity and save them in the dictionary metric_dict\n",
    "\n",
    "   \n",
    "    return metric_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Training loop\n",
    "A training loop is the repetitive process in which a model learns from data by iteratively passing batches through the network, calculating loss, updating weights using an optimizer, and repeating until the model converges or a set number of iterations is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "final_model_path = os.path.join(os.path.dirname(os.path.realpath('__file__')),f'finalModel_{NUM_EPOCHS}_Epochs_size_{size}.model')\n",
    "best_model_path = os.path.join(os.path.dirname(os.path.realpath('__file__')),f'bestModel_{NUM_EPOCHS}_Epochs_size_{size}.model')\n",
    "\n",
    "# to save performances \n",
    "writer_dict = {'epoch': [],\n",
    "               'loss_train': [],\n",
    "               'loss_valid': [],\n",
    "               'lr': [],\n",
    "               'Acc': [],\n",
    "               'F1': [],\n",
    "               'Sensitivity': [],\n",
    "               'Specificity': [],\n",
    "               'AUC': []}\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "min_valid_loss = None\n",
    "scaler = torch.amp.GradScaler() \n",
    "# https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html\n",
    "# Gradient scaling helps prevent gradients with small magnitudes from flushing to zero (“underflowing”) when training with mixed precision.\n",
    "\n",
    "for epoch in range(NUM_EPOCHS): # iterate over the epochs\n",
    "\n",
    "    start_time = time.time()\n",
    "  \n",
    "    model.train()\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.autocast(device_type='cuda'):\n",
    "        \n",
    "            inputs = batch['image'].to(device) # move to gpu\n",
    "            targets = ... # get labels saved in batch\n",
    "        \n",
    "            outputs = ... # calculate the predicted output by giving your model the input images\n",
    "                    \n",
    "            loss = criterion(...)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss.append(loss.item())\n",
    "    \n",
    "    lr = optimizer.param_groups[0]['lr'] # to get actual learning rate (only important if learning rate is not constant over time)\n",
    "    writer_dict['lr'].append(lr) \n",
    "    \n",
    "    if epoch == NUM_EPOCHS-1: # check if last epoch is reached\n",
    "        # You can use this code to save your model\n",
    "        state_dict = model.state_dict()\n",
    "        for key in state_dict.keys():\n",
    "            state_dict[key] = state_dict[key].cpu()\n",
    "\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'save_dir': final_model_path,\n",
    "            'state_dict': state_dict,\n",
    "            'optimizer_state_dict': optimizer.state_dict()},\n",
    "            final_model_path)\n",
    "\n",
    "    # Now we want to evaluate our model on the validation data\n",
    "    model.eval()\n",
    "    y_true = torch.tensor([]).to(device)\n",
    "    y_score = torch.tensor([]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            with torch.autocast(device_type='cuda'):\n",
    "\n",
    "                inputs = ...\n",
    "                targets = ...\n",
    "                outputs = ...\n",
    "            \n",
    "            val_loss.append(...)\n",
    "\n",
    "            y_true = torch.cat((y_true, targets), 0)\n",
    "            y_score = torch.cat((y_score, outputs.sigmoid()), 0) # sigmoid to transform outputs from logits to probabilites \n",
    "    \n",
    "    y_true = y_true.cpu().numpy() # move from gpu to cpu and convert tensor to numpy array\n",
    "    y_score = y_score.detach().cpu().numpy()\n",
    "\n",
    "    \n",
    "    # Save best model on validation set \n",
    "    if epoch == 0:\n",
    "        min_valid_loss = np.mean(val_loss)\n",
    "\n",
    "        ... # save model\n",
    "\n",
    "       \n",
    "    elif np.mean(val_loss) < min_valid_loss:\n",
    "        min_valid_loss = np.mean(val_loss)\n",
    "\n",
    "         ... # save model\n",
    "\n",
    "    # to monitor model performance  \n",
    "    writer_dict['epoch'].append(...)\n",
    "    writer_dict['loss_train'].append(...)\n",
    "    writer_dict['loss_valid'].append(...)\n",
    "\n",
    "    # eval your model usign the evaluation function from task 3\n",
    "    metric_dict_val = ... \n",
    "\n",
    "    for cur_key in metric_dict_val.keys():\n",
    "        writer_dict[cur_key].append(metric_dict_val[cur_key])\n",
    "    \n",
    "    end_time = time.time()\n",
    " \n",
    "    print('Epoch %03d, time for epoch: %3.2f' %(epoch, end_time-start_time))\n",
    "    print('loss %2.4f, validation loss %2.4f' %(...))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Final model evaluation\n",
    "- Plot the loss function and the determined evaluation metrics\n",
    "- Write an evaluation / inference function to evaluate the trained model on the test set\n",
    "- Check the false-positive and false negative cases\n",
    "\n",
    "Take a closer look at the results of the model. Are there any reasons why the model was wrong here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss saved in writer_dict\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot evaluation metrics saved in writer_dict\n",
    "plt.figure()\n",
    "plt.plot(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save writer_dict \n",
    "# see: https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict-or-any-other-python-object\n",
    "import pickle\n",
    "\n",
    "savePath = os.path.join(os.path.dirname(os.path.realpath('__file__')),f'writer_dict_{NUM_EPOCHS}_Epochs_size_{size}.pickle')\n",
    "\n",
    "# write dictionary\n",
    "with open(...) as f:\n",
    "    ...\n",
    "# load dictionary\n",
    "with open(...) as f:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model inference\n",
    "Now we want to use our trained model to predict new data.\\\n",
    "Therefore, we define a function *infer*, which gets a data loader and the trained model as input. \\\n",
    "You can use the code from the training loop for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(...):\n",
    "    model.eval()\n",
    "    y_true = torch.tensor([]).to(device)\n",
    "    y_score = torch.tensor([]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ...\n",
    "\n",
    "\n",
    "        metric_dict = evalModel(...)\n",
    "\n",
    "        y_pred = (y_score >= 0.5).astype(int)\n",
    "\n",
    "        print('auc: %.3f  acc:%.3f, f1: %3f, sens.: %3f, spec.: %3f' %(metric_dict['AUC'],metric_dict['Acc'], metric_dict['F1'],metric_dict['Sensitivity'], metric_dict['Specificity']))\n",
    "        \n",
    "\n",
    "    return metric_dict, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use the code belwo to load the best model\n",
    "model = resnet18(spatial_dims=2,num_classes=1,n_input_channels=1).to(device)\n",
    "checkpoint = torch.load(best_model_path)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "model.eval()\n",
    "print('==> Evaluating ...')\n",
    "print('Validation performance:')\n",
    "metrics_val, val_pred = infer(...)\n",
    "print('Test performance')\n",
    "metrics_test, test_pred = infer(...)\n",
    "\n",
    "# Now we want to convert our dictionaries to a pandas dataframe and save the results as csv file\n",
    "# see https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.from_dict.html\n",
    "import pandas as pd\n",
    "df_test = pd.DataFrame(...)\n",
    "df_test.to_csv(...)\n",
    "\n",
    "df_val = pd.DataFrame(...)\n",
    "df_val.to_csv(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to have a closer look on our model performance and the predicted results from our test set. \\\n",
    "Write code, to visualize false positive (prediction = Pneumonia, label = Healthy) and false negative cases (prediction = Healthy, label = Pneumonia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check false positive and false negative test cases\n",
    "# save indices for fp and fn in a separate list\n",
    "fp_cases = []\n",
    "fn_cases = []\n",
    "for cur_ind in range(len(test_dataset)):\n",
    "    if test_pred[cur_ind] == 1 and test_dataset.labels[cur_ind]==0:\n",
    "        ...\n",
    "    elif test_pred[cur_ind] == 0 and test_dataset.labels[cur_ind]==1:\n",
    "        ...\n",
    "\n",
    "# plot fp and fn cases using plt.subplots\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Tasks\n",
    "When you have finished everything else, you can continue working on one of the following tasks, for example, or come up with something of your own\n",
    "## Task 1: Multi-label classification\n",
    "So far we have only dealt with a binary classification problem. However, we often also have to deal with multi-label problems. Multi-label classification is a type of classification where each data point can belong to multiple classes simultaneously, meaning the model predicts a set of labels rather than just one. For example, a photo might be labeled as both \"cat\" and \"dog\" if it contains both animals.\n",
    "You can use and adapt the code you have developed here to solve a multi-label classification problem using the ChestMNIST from the [MedMnist](https://medmnist.com/) data set.\n",
    "\n",
    "## Task 2: Large Language Models\n",
    "Use Large Language Models (LLMs) to generate radiological reports for the PneumoniaMNIST dataset. See for example  https://huggingface.co/microsoft/Phi-3-vision-128k-instruct\n",
    "\n",
    "## Task 3: Generative data augmentation\n",
    "For generative data augmentation you can for example train a conditional Generative Adversarial Network (GAN) to generate new healty and new pneumonia images, see for example: https://github.com/qbxlvnf11/conditional-GAN/blob/main/conditional-GAN-generating-fashion-mnist.ipynb \n",
    "GANs are relatively difficult to train, which is why you can also consider using a pre-trained GAN to achieve better results. You can take a look at this: GANs are relatively difficult to train, which is why you can also consider using a pre-trained GAN to achieve better results. You can take a look at this: https://github.com/Project-MONAI/tutorials/blob/main/modules/mednist_GAN_tutorial.ipynb.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "namt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
